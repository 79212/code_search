{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisite: Familiarize yourself with sequence-to-sequence models\n",
    "\n",
    "If you are not familiar with sequence to sequence models, please refer to [this tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8).\n",
    "\n",
    "### Pre-Requisite: Make Sure you have the right files prepared from Step 1\n",
    "\n",
    "You should have these files in the root of the `./data/processed_data/` directory:\n",
    "\n",
    "1. `{train/valid/test.function}` - these are python function definitions tokenized (by space), 1 line per function.\n",
    "2. `{train/valid/test.docstring}` - these are docstrings that correspond to each of the python function definitions, and have a 1:1 correspondence with the lines in *.function files.\n",
    "3. `{train/valid/test.lineage}` - every line in this file contains a link back to the original location (github repo link) where the code was retrieved.  There is a 1:1 correspondence with the lines in this file and the other two files. This is useful for debugging.\n",
    "\n",
    "\n",
    "### Set the value of `use_cache` appropriately.  \n",
    "\n",
    "If `use_cache = True`, data will be downloaded where possible instead of re-computing.  However, it is highly recommended that you set `use_cache = False` for this tutorial as it will be less confusing, and you will learn more by runing these steps yourself. **This notebook takes approximately 4 hours to run on an AWS `p3.8xlarge` instance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will allow the notebook to run faster\n",
    "from pathlib import Path\n",
    "from general_utils import get_step2_prerequisite_files, read_training_files\n",
    "from keras.utils import get_file\n",
    "OUTPUT_PATH = Path('./data/seq2seq/')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text From File\n",
    "\n",
    "We want to read in raw text from files so we can pre-process the text for modeling as described in [this tutorial](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Num rows for encoder training + validation input: 1,227,989\n",
      "WARNING:root:Num rows for encoder holdout input: 177,220\n",
      "WARNING:root:Num rows for decoder training + validation input: 1,227,989\n",
      "WARNING:root:Num rows for decoder holdout input: 177,220\n"
     ]
    }
   ],
   "source": [
    "if use_cache:\n",
    "    get_step2_prerequisite_files(output_directory = './data/processed_data')\n",
    "\n",
    "# you want to supply the directory where the files are from step 1.\n",
    "train_code, holdout_code, train_comment, holdout_comment = read_training_files('./data/processed_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and comment files should be of the same length.\n",
    "\n",
    "assert len(train_code) == len(train_comment)\n",
    "assert len(holdout_code) == len(holdout_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 55 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 111 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 34 sec\n",
      "WARNING:root:Finished parsing 1,223,937 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 31 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 15 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 35 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 11 sec\n",
      "WARNING:root:Finished parsing 1,223,937 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 13 sec\n"
     ]
    }
   ],
   "source": [
    "from ktext.preprocess import processor\n",
    "\n",
    "if not use_cache:    \n",
    "    code_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
    "    t_code = code_proc.fit_transform(train_code)\n",
    "\n",
    "    comment_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=14000, padding ='post')\n",
    "    t_comment = comment_proc.fit_transform(train_comment)\n",
    "\n",
    "elif use_cache:\n",
    "    logging.warning('Not fitting transform function because use_cache=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "if not use_cache:\n",
    "    # Save the preprocessor\n",
    "    with open(OUTPUT_PATH/'py_code_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(code_proc, f)\n",
    "\n",
    "    with open(OUTPUT_PATH/'py_comment_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(comment_proc, f)\n",
    "\n",
    "    # Save the processed data\n",
    "    np.save(OUTPUT_PATH/'py_t_code_vecs_v2.npy', t_code)\n",
    "    np.save(OUTPUT_PATH/'py_t_comment_vecs_v2.npy', t_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrange data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1223937, 55)\n",
      "Shape of decoder input: (1223937, 14)\n",
      "Shape of decoder target: (1223937, 14)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
    "\n",
    "\n",
    "encoder_input_data, encoder_seq_len = load_encoder_inputs(OUTPUT_PATH/'py_t_code_vecs_v2.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs(OUTPUT_PATH/'py_t_comment_vecs_v2.npy')\n",
    "num_encoder_tokens, enc_pp = load_text_processor(OUTPUT_PATH/'py_code_proc_v2.dpkl')\n",
    "num_decoder_tokens, dec_pp = load_text_processor(OUTPUT_PATH/'py_comment_proc_v2.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have the above files on disk because you set `use_cache = True` you can download the files for the above function calls here:\n",
    "\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_t_code_vecs_v2.npy\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_t_comment_vecs_v2.npy\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl\n",
    " - https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Seq2Seq Model For Summarizing Code\n",
    "\n",
    "We will build a model to predict the docstring given a function or a method.  While this is a very cool task in itself, this is not the end goal of this exercise.  The motivation for training this model is to learn a general purpose feature extractor for code that we can use for the task of code search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import build_seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convenience function `build_seq2seq_model` constructs the architecture for a sequence-to-sequence model.  \n",
    "\n",
    "The architecture built for this tutorial is a minimal example with only one layer for the encoder and decoder, and does not include things like [attention](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf).  We encourage you to try and build different architectures to see what works best for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_Model = build_seq2seq_model(word_emb_dim=800,\n",
    "                                    hidden_state_dim=1200,\n",
    "                                    encoder_seq_len=encoder_seq_len,\n",
    "                                    num_encoder_tokens=num_encoder_tokens,\n",
    "                                    num_decoder_tokens=num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 800)    11201600    Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 800)    3200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 1200)         23208400    Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 1200), 7203600     Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 1200)   4800        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 14002)  16816402    Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 58,438,002\n",
      "Trainable params: 58,432,402\n",
      "Non-trainable params: 5,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning:\n",
    "\n",
    "if Setting `use_cache = False` this next part takes 4 days to train on AWS on a `p3.8xlarge` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1077064 samples, validate on 146873 samples\n",
      "Epoch 1/15\n",
      "1077064/1077064 [==============================] - 651s 604us/step - loss: 3.9441 - val_loss: 3.3994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model_2/Encoder-Last-GRU/while/Exit_2:0' shape=(?, 1200) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "1077064/1077064 [==============================] - 646s 600us/step - loss: 3.1653 - val_loss: 3.0774\n",
      "Epoch 3/15\n",
      "1077064/1077064 [==============================] - 647s 600us/step - loss: 2.8911 - val_loss: 2.9077\n",
      "Epoch 4/15\n",
      "1077064/1077064 [==============================] - 647s 601us/step - loss: 2.7226 - val_loss: 2.8068\n",
      "Epoch 5/15\n",
      "1077064/1077064 [==============================] - 647s 601us/step - loss: 2.6031 - val_loss: 2.7404\n",
      "Epoch 6/15\n",
      "1077064/1077064 [==============================] - 647s 601us/step - loss: 2.5107 - val_loss: 2.6943\n",
      "Epoch 7/15\n",
      "1077064/1077064 [==============================] - 647s 601us/step - loss: 2.4352 - val_loss: 2.6562\n",
      "Epoch 8/15\n",
      "1077064/1077064 [==============================] - 647s 600us/step - loss: 2.3715 - val_loss: 2.6287\n",
      "Epoch 9/15\n",
      "1077064/1077064 [==============================] - 647s 600us/step - loss: 2.3165 - val_loss: 2.6087\n",
      "Epoch 10/15\n",
      "1077064/1077064 [==============================] - 647s 601us/step - loss: 2.2679 - val_loss: 2.5947\n",
      "Epoch 11/15\n",
      "1077064/1077064 [==============================] - 647s 601us/step - loss: 2.2245 - val_loss: 2.5869\n",
      "Epoch 12/15\n",
      "1077064/1077064 [==============================] - 647s 601us/step - loss: 2.1848 - val_loss: 2.5726\n",
      "Epoch 13/15\n",
      "1077064/1077064 [==============================] - 647s 601us/step - loss: 2.1485 - val_loss: 2.5718\n",
      "Epoch 14/15\n",
      "1077064/1077064 [==============================] - 647s 601us/step - loss: 2.1152 - val_loss: 2.5649\n",
      "Epoch 15/15\n",
      "1077064/1077064 [==============================] - 647s 601us/step - loss: 2.0839 - val_loss: 2.5629\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "if not use_cache:\n",
    "\n",
    "    from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "    import numpy as np\n",
    "    from keras import optimizers\n",
    "\n",
    "    seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.00005), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    script_name_base = 'py_func_sum_v7_'\n",
    "    csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                       save_best_only=True)\n",
    "\n",
    "    batch_size = 1100\n",
    "    epochs = 15\n",
    "    history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cache:\n",
    "    logging.warning('Not re-training function summarizer seq2seq model because use_cache=True')\n",
    "    # Load model from url\n",
    "    loc = get_file(fname='py_func_sum_v7_.epoch15-val2.55631.hdf5',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_func_sum_v7_.epoch15-val2.55631.hdf5')\n",
    "    seq2seq_Model = load_model(loc)\n",
    "    \n",
    "    # Load encoder (code) pre-processor from url\n",
    "    loc = get_file(fname='py_code_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl')\n",
    "    num_encoder_tokens, enc_pp = load_text_processor(loc)\n",
    "    \n",
    "    # Load decoder (docstrings/comments) pre-processor from url\n",
    "    loc = get_file(fname='py_comment_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl')\n",
    "    num_decoder_tokens, dec_pp = load_text_processor(loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above procedure will automatically download a pre-trained model and associated artifacts from https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/ if `use_cache = True`.  \n",
    "\n",
    "Otherwise, the above code will checkpoint the best model after each epoch into the current directory with prefix `py_func_sum_v7_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Seq2Seq Model For Code Summarization\n",
    "\n",
    "To evaluate this model we are going to do two things:\n",
    "\n",
    "1.  Manually inspect the results of predicted docstrings for code snippets, to make sure they look sensible.\n",
    "2.  Calculate the BLEU Score so that we can quantitately benchmark different iterations of this algorithm and to guide hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Inspect Results (on holdout set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 57624 =================\n",
      "\n",
      "Original Input:\n",
      " def FRiP workflow conf for t in conf sample_targets if conf frip reads t _5000000_nochrM bam else reads t _4000000_nochrM bam frip attach_back workflow ShellCommand fr bedtools intersect f param p wa u abam input reads b input peaks bed wc l total samtools flagstat input reads head 1 cut d f1 echo fr total output frip tool intersectBed input reads reads if conf down else t _nochrM bam peaks conf prefix _sort_peaks narrowPeak if conf get macs2 type in both narrow else conf prefix _b_sort_peaks broadPeak output frip t frip param p 1E 9 name FRiP score frip allow_fail True frip allow_dangling True frip update param conf items bedtools stat_frip workflow conf if conf long tex_frip workflow conf\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"fraction of reads in peaks regions at 4 m reads level for example : 2 treat , 2 control modify : without down sampling read peaks calling , use merged peaks for comparison\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " reads in fastq files from input bam file\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 86868 =================\n",
      "\n",
      "Original Input:\n",
      " def all_estimates reviews k 1 reviews reviews astype float k 1 nusers nmovies reviews shape estimates np zeros_like reviews for u in range nusers ureviews np delete reviews u axis 0 ureviews ureviews mean 0 ureviews ureviews std 0 1e 05 ureviews ureviews T copy for m in np where reviews u 0 0 estimates u m nn_movie ureviews reviews u m k return estimates\n",
      " \n",
      "\n",
      "Original Output:\n",
      " estimate all review ratings\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " estimates the average of all ratings for a given time\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 5860 =================\n",
      "\n",
      "Original Input:\n",
      " def items self raise NotImplementedError\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get all the items of the configuration ( key / value pairs ) . : return tuple : the items of the configuration .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return a list of items in the queue\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 146052 =================\n",
      "\n",
      "Original Input:\n",
      " def reaction_charge reaction compound_charge charge_sum 0 0 for compound value in reaction compounds charge compound_charge get compound name float nan charge_sum charge float value return charge_sum\n",
      " \n",
      "\n",
      "Original Output:\n",
      " calculate the overall charge for the specified reaction .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " reaction charge of reaction charge of reaction\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 25466 =================\n",
      "\n",
      "Original Input:\n",
      " def set_predictors self predictors self predictors predictors\n",
      " \n",
      "\n",
      "Original Output:\n",
      " set the columns to be used predictors ( also called independent variables or features ) of the model .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " set predictors for predictors\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 73418 =================\n",
      "\n",
      "Original Input:\n",
      " def add_key user key filename options OPTIONS key PublicKey key with LockedAtomicFile filename autobreak True as f for line in f if key key in line raise PublicKeyExists public key already exists f write line if s in options options user print options key file f f commit return True\n",
      " \n",
      "\n",
      "Original Output:\n",
      " add a key to the authorized_keys file .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " add a key to the user s private key\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1565 =================\n",
      "\n",
      "Original Input:\n",
      " def two_sat_scc clauses g Graph build directed True for clause in clauses left right clause g add_edge left right g add_edge right left connected_components scc g for component in connected_components vertices set component for vertex in vertices if vertex in vertices return False return True\n",
      " \n",
      "\n",
      "Original Output:\n",
      " solves the constraint satisfaction ( csp ) using kosaraju 's scc detection algorithm .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns true if two edges are semantically feasible\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 79420 =================\n",
      "\n",
      "Original Input:\n",
      " def call self inputs kwargs print Inputs 0 shape s str inputs 0 shape print Inputs 1 shape s str inputs 1 shape l1 inputs 0 l2 inputs 1 output K batch_dot inputs 0 inputs 1 axes 1 1 return output\n",
      " \n",
      "\n",
      "Original Output:\n",
      " layer logic .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " run the model\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 927 =================\n",
      "\n",
      "Original Input:\n",
      " register filter def integer value try return int value except ValueError return value\n",
      " \n",
      "\n",
      "Original Output:\n",
      " returns the given value as an int type .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns the integer value of a given integer\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 22129 =================\n",
      "\n",
      "Original Input:\n",
      " property def string self return ffi string C git_refspec_string self _refspec decode\n",
      " \n",
      "\n",
      "Original Output:\n",
      " string which was used to create this refspec\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " string representation of the object\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 107556 =================\n",
      "\n",
      "Original Input:\n",
      " def clear self age None n date_now for p in glob glob os path join self path if age is None or n date_modified p days age os unlink p\n",
      " \n",
      "\n",
      "Original Output:\n",
      " clears all items from the cache ( whose age is the given amount of days or older ) .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " clear all the age of the age days\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 81706 =================\n",
      "\n",
      "Original Input:\n",
      " contextlib contextmanager def logging_reduced module_name None level logging CRITICAL logger logging getLogger module_name old_level logger getEffectiveLevel logger setLevel level yield logger setLevel old_level\n",
      " \n",
      "\n",
      "Original Output:\n",
      " temporarly reduce the logging level of a specific module or globally if none\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " context manager to temporarily change the logging level of the logger\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 168141 =================\n",
      "\n",
      "Original Input:\n",
      " def flip_sort stack flips sstack sorted stack reverse True stack list stack 1 for i in range len stack 1 if stack i sstack i continue pos find_pos stack sstack i if pos len stack 1 flips append pos 1 iflip stack pos 1 flips append i 1 iflip stack i 1 return flips\n",
      " \n",
      "\n",
      "Original Output:\n",
      " while not in order : 1 - find biggest pancake not in position 2 - flip it to the top of the stack ( if not already in it ) 3 - flip the top of the stack into pancake correct positon\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " flip the stack of the stack\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 65203 =================\n",
      "\n",
      "Original Input:\n",
      " defer inlineCallbacks def test_huge_lines self yield self lbf append 12 32768 yield self lbf flush self assertCallbacks 12 2048 n 16\n",
      " \n",
      "\n",
      "Original Output:\n",
      " huge lines are split\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " lines are n t allowed\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 79570 =================\n",
      "\n",
      "Original Input:\n",
      " def test_fuzzy_snippet_file_finder self completer SnippetFileCompleter config matched_files completer fuzzyfinder hgfile completer collection should_match test snippet hg y4yKd file txt test snippet hg y4yKd hg_file1 txt self assertEqual matched_files should_match\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test snippet file fuzzy finder completer\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test fuzzy finder with fuzzy finder\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "import pandas as pd\n",
    "\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=enc_pp,\n",
    "                                 decoder_preprocessor=dec_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "\n",
    "demo_testdf = pd.DataFrame({'code':holdout_code, 'comment':holdout_comment, 'ref':''})\n",
    "seq2seq_inf.demo_model_predictions(n=15, df=demo_testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on manual inspection of results:\n",
    "\n",
    "The predicted code summaries are not perfect, but we can see that the model has learned to extract some semantic meaning from the code.  That's all we need to get reasonable results in this case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BLEU Score (on holdout set)\n",
    "\n",
    "BLEU Score is described [in this wikipedia article](https://en.wikipedia.org/wiki/BLEU), and is a way to measure the efficacy of summarization/translation such as the one we conducted here.  This metric is useful if you wish to conduct extensive hyper-parameter tuning and try to improve the seq2seq model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generating predictions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e0663e88db4cdfb90e7680aef26329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=177220), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Calculating BLEU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07134319200840655"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will return a BLEU Score\n",
    "seq2seq_inf.evaluate_model(input_strings=holdout_code, \n",
    "                           output_strings=holdout_comment, \n",
    "                           max_len=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model to disk\n",
    "\n",
    "Save the model to disk so you can use in Step 4 of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model_2/Encoder-Last-GRU/while/Exit_2:0' shape=(?, 1200) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.save(OUTPUT_PATH/'code_summary_seq2seq_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
